{
  "project": "office2pdf",
  "branchName": "ralph/phase10-perf-infra",
  "description": "Phase 10: Performance & Infrastructure - Per-stage instrumentation, hotspot optimization, multi-threaded compression, performance tiers, REST API server mode",
  "userStories": [
    {
      "id": "US-090",
      "title": "Add per-stage timing and memory instrumentation",
      "description": "The conversion pipeline has 4 stages (OOXML parse → IR → Typst codegen → PDF compile) but there's no visibility into per-stage performance. This makes it impossible to identify bottlenecks.\n\nAdd instrumentation:\n1. Create a `ConvertMetrics` struct that captures per-stage timing:\n   - `parse_duration: Duration` — time spent in parser (DOCX/PPTX/XLSX)\n   - `codegen_duration: Duration` — time spent in Typst codegen\n   - `compile_duration: Duration` — time spent in Typst compile + PDF export\n   - `total_duration: Duration` — end-to-end\n   - `input_size_bytes: u64` — input file size\n   - `output_size_bytes: u64` — output PDF size\n   - `page_count: u32` — number of pages in output\n2. Add `metrics: Option<ConvertMetrics>` to `ConvertResult`\n3. Instrument each stage in the conversion pipeline with `std::time::Instant`\n4. Return metrics in `ConvertResult` when conversion succeeds\n5. Add `--metrics` CLI flag to print timing info to stderr\n6. Use `std::time::Instant` (no external dependencies)\n\nThis is the foundation for Phase 10's performance optimization work.",
      "acceptanceCriteria": [
        "ConvertMetrics struct captures parse, codegen, compile durations",
        "ConvertMetrics includes input/output sizes and page count",
        "ConvertResult includes Optional<ConvertMetrics>",
        "Each pipeline stage is timed with std::time::Instant",
        "CLI --metrics flag prints timing breakdown to stderr",
        "Unit test: conversion returns populated ConvertMetrics",
        "Test: total_duration >= parse + codegen + compile durations",
        "cargo test --workspace passes",
        "cargo fmt --all -- --check passes",
        "cargo clippy --workspace -- -D warnings passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Read the conversion pipeline in lib.rs (convert(), convert_with_options(), convert_bytes()) to understand where to insert timing points. Each stage is already separate — wrap them with Instant::now() / elapsed(). Add ConvertMetrics to ir/document.rs or a new metrics module. Keep it simple — Duration fields, no allocations. The --metrics flag for CLI should use eprintln! to print to stderr so it doesn't interfere with PDF output."
    },
    {
      "id": "US-091",
      "title": "Optimize XML parsing hotspots in PPTX and DOCX parsers",
      "description": "Based on the PRD performance target of <1s for 10-page documents (currently ~3.5s), XML parsing is likely the primary bottleneck.\n\nProfile and optimize:\n1. Use the instrumentation from US-090 to identify which stage is slowest\n2. Common XML parsing hotspots:\n   - PPTX: Repeated re-parsing of relationship files (.rels) for each slide — cache them\n   - PPTX: Theme/master/layout inheritance chain being walked per element — precompute\n   - DOCX: docx-rs + serde_json roundtrip — check if direct field access is possible\n   - XLSX: umya-spreadsheet loading entire workbook into memory — check if lazy loading is possible\n3. Apply targeted optimizations:\n   - Cache parsed XML data that's accessed multiple times\n   - Reduce string allocations (use &str references where possible)\n   - Avoid re-parsing the same ZIP entries\n   - Use quick-xml's zero-copy parsing where applicable\n4. Measure improvement with before/after benchmarks on reference fixtures\n\nTarget: reduce total conversion time by at least 30% for 10-page documents.",
      "acceptanceCriteria": [
        "Per-stage timing data identifies the primary bottleneck stage",
        "At least 2 specific optimizations are applied to the identified bottleneck",
        "Repeated XML re-parsing is eliminated via caching where applicable",
        "Before/after timing comparison shows measurable improvement",
        "10-page document conversion time reduced by at least 30%",
        "No functional regressions (all existing tests pass)",
        "Performance improvement documented in progress.txt with specific numbers",
        "cargo test --workspace passes",
        "cargo fmt --all -- --check passes",
        "cargo clippy --workspace -- -D warnings passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Start with the US-090 instrumentation to identify which stage is slowest. Common wins: (1) Cache .rels files in PPTX parser — they're parsed every time a relationship is resolved. (2) Pre-parse theme colors/fonts once instead of per-slide. (3) In DOCX, the serde_json roundtrip through docx-rs is expensive — check if there's a way to access fields directly. (4) Avoid String::clone() in hot paths — use references or Cow<str>. Profile with `cargo test` timing or simple Instant benchmarks."
    },
    {
      "id": "US-092",
      "title": "Enable multi-threaded PDF compression via rayon",
      "description": "The Typst PDF backend (typst-pdf) may support parallel compression via rayon. Additionally, if multiple documents are being converted (batch mode), conversions themselves could be parallelized.\n\nInvestigate and implement:\n1. Check if typst-pdf or its underlying PDF writer supports rayon or parallel stream compression\n2. If typst-pdf already uses rayon internally, ensure it's enabled in our dependency config\n3. If not, check if krilla (typst's new PDF backend) supports parallel compression\n4. For batch conversion in CLI (`--outdir` mode), use rayon to convert multiple files in parallel:\n   - `rayon::ThreadPoolBuilder::new().num_threads(N).build_global()`\n   - Use `.par_iter()` on the list of input files\n   - Report progress per file\n5. Add `--jobs N` or `-j N` CLI flag to control parallelism (default: number of CPU cores)\n6. Add rayon as a dependency if not already present\n\nSince office2pdf is thread-safe by design (no global state, no LibreOffice dependency), true parallelism is achievable — this is a major differentiator vs. LibreOffice-based solutions.",
      "acceptanceCriteria": [
        "Batch conversion uses rayon for parallel file processing",
        "CLI --jobs N flag controls thread pool size",
        "Default parallelism is number of CPU cores",
        "Single-file conversion is not negatively impacted",
        "Batch conversion of 10 files is at least 2x faster than sequential on multi-core",
        "Unit test: batch conversion with --jobs 2 produces correct output for all files",
        "cargo test --workspace passes",
        "cargo fmt --all -- --check passes",
        "cargo clippy --workspace -- -D warnings passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Rayon is a well-known Rust parallelism crate and is justified as a dependency for this use case. Check if rayon is already in the dependency tree (typst may depend on it). The key insight is that each file conversion is fully independent — no shared state — so par_iter() is straightforward. Be careful with error handling in parallel context — use rayon's error collection patterns."
    },
    {
      "id": "US-093",
      "title": "Define tiered performance targets with reference benchmarks",
      "description": "The PRD has a single performance target (<1s for 10 pages) that doesn't account for document complexity or format differences. The perf tests currently use 3s budget and still fail.\n\nDefine realistic, tiered performance targets:\n1. Create reference benchmark documents for each tier:\n   - **Small** (<10 pages): simple text-only DOCX, 5-slide PPTX, 3-sheet XLSX\n   - **Medium** (10-50 pages): mixed content DOCX with tables/images, 20-slide PPTX, 10-sheet XLSX\n   - **Large** (50-100 pages): complex DOCX, 50-slide PPTX, large XLSX with formatting\n2. Define target times per tier (based on actual measurements after US-091 optimizations):\n   - Small: P50 < 1s, P95 < 2s\n   - Medium: P50 < 3s, P95 < 5s\n   - Large: P50 < 5s, P95 < 10s\n3. Update `crates/office2pdf/tests/perf_validation.rs` to use tiered targets\n4. Generate synthetic test documents programmatically (don't rely on external fixtures)\n5. Run benchmarks on CI and document baseline numbers\n6. Update `PRD.md` performance section if needed to reflect realistic targets",
      "acceptanceCriteria": [
        "Three performance tiers defined (small/medium/large) with specific page counts",
        "Synthetic test documents generated programmatically for each tier",
        "perf_validation.rs tests use tiered targets instead of single global target",
        "At least small-tier tests pass on local hardware",
        "Baseline performance numbers documented in progress.txt",
        "perf tests use realistic budgets based on actual measurements",
        "cargo test --workspace passes (including updated perf tests)",
        "cargo fmt --all -- --check passes",
        "cargo clippy --workspace -- -D warnings passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "The current perf test budget of 3s (perf_validation.rs:11) still fails at ~3.5-4.0s. After US-091 optimizations, re-measure and set realistic targets. For synthetic documents, use docx-rs to generate DOCX, ZIP+quick-xml for PPTX, and umya-spreadsheet for XLSX. P95 budgets should include CI variance. Consider using #[ignore] for large tier tests that may not pass on GitHub Actions runners."
    },
    {
      "id": "US-094",
      "title": "Add HTTP server mode with REST API (office2pdf serve)",
      "description": "Inspired by Gotenberg's REST API, add an HTTP server mode to office2pdf for integration into microservice architectures.\n\nImplement `office2pdf serve`:\n1. Add a `serve` subcommand to the CLI (`crates/office2pdf-cli/src/main.rs`)\n2. Use a lightweight HTTP server (consider `tiny_http` or `axum` — prefer the simpler option)\n3. Endpoints:\n   - `POST /convert` — accepts multipart/form-data with file upload, returns PDF bytes\n   - `GET /health` — returns 200 OK with version info\n   - `GET /formats` — returns supported input formats\n4. Request format: multipart/form-data with `file` field (the Office document)\n5. Optional query parameters: `format` (override auto-detection), `paper` (paper size), `landscape` (boolean)\n6. Response: `application/pdf` body with appropriate headers\n7. Error responses: JSON with error message and status code\n8. Configuration: `--port N` (default 3000), `--host` (default 127.0.0.1)\n\nKeep it minimal — no auth, no webhooks, no queue. Just a clean REST API for document conversion.\n\nThis should be behind a feature flag (`server`) to keep the default binary small.",
      "acceptanceCriteria": [
        "office2pdf serve --port 3000 starts an HTTP server",
        "POST /convert accepts multipart/form-data with file upload and returns PDF",
        "GET /health returns 200 with version info",
        "GET /formats returns JSON list of supported formats",
        "Error responses are JSON with meaningful messages",
        "--port and --host flags configure the server",
        "Server feature is behind a cargo feature flag",
        "Integration test: POST a DOCX file, receive valid PDF response",
        "cargo test --workspace passes",
        "cargo fmt --all -- --check passes",
        "cargo clippy --workspace -- -D warnings passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Consider tiny_http for minimal dependency footprint (pure Rust, no tokio needed). The multipart parsing can use the `multipart` crate or a simple manual parser. The server feature should be optional: `[features] server = ['tiny_http']` in Cargo.toml. Keep the implementation in a separate module (e.g., `crates/office2pdf-cli/src/server.rs`). This is a thin HTTP wrapper around the existing convert_bytes() API."
    }
  ]
}
